---
- name: Setup and run tests
  hosts: test_runner
  vars:
    distrobox_name: assisted-capi-providers
    kind_version: v0.25.0
    cert_manager_version: v1.5.3
    remote_manifests_path: /tmp/manifests
    test_namespace: test-capi
    number_of_nodes: "{{ lookup('ansible.builtin.env', 'NUMBER_OF_NODES', default='7') }}"
    cluster_topology: "{{ lookup('ansible.builtin.env', 'CLUSTER_TOPOLOGY', default='multinode') }}"
    example_manifest: "{{ cluster_topology }}-example.yaml"


    capi_version: "{{ lookup('ansible.builtin.env', 'CAPI_VERSION', default='v1.9.4') }}"
    capm3_version: "{{ lookup('ansible.builtin.env', 'CAPM3_VERSION', default='v1.9.2') }}"
    dist_dir: "{{ lookup('ansible.builtin.env', 'DIST_DIR') }}"
    src_dir: "/tmp/capbcoa"
    kind_cluster_name: capi-baremetal-provider
    ssh_authorized_key: "{{ lookup('ansible.builtin.env', 'SSH_AUTHORIZED_KEY') }}"
    pullsecret: "{{ lookup('ansible.builtin.env', 'PULLSECRET') }}"
    container_tag: "{{ lookup('ansible.builtin.env', 'CONTAINER_TAG', default='local')}}"
    upgrade_to_version: "4.19.0-ec.3"
    cluster_name: "test-{{cluster_topology}}"
  tasks:
  - name: Distribution
    debug: msg="{{ ansible_distribution }}"
  - name: Install EPEL
    ansible.builtin.dnf:
      enablerepo: powertools
      name:
      - epel-release
    when: ansible_distribution != "RedHat"
  - name: Install dependencies
    command: dnf --enablerepo=crb -y install git make podman qemu-kvm libvirt virt-install containernetworking-plugins
    when: ansible_distribution != "RedHat"
  - name: Install dependencies RHEL
    command: dnf -y install git make podman qemu-kvm libvirt virt-install containernetworking-plugins
    when: ansible_distribution == "RedHat"
  - name: enable libvirtd
    command: systemctl enable --now libvirtd

  - name: copy src
    synchronize:
      src: ../../../
      dest: "{{ src_dir }}"
      archive: true
      recursive: true

  - name: Detect internal IP
    shell: hostname -i | grep -oE "10\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}" | head -n1 > /tmp/internal-ip
  - name: Show internal IP
    shell: cat /tmp/internal-ip
    register: internalip
  - name: Debug tests output
    debug: "msg={{ internalip.stdout }}"

  - name: change default dns_servers for containers
    shell: sed -i 's/#dns_servers = \[\]/dns_servers = \["192.168.222.1", "8.8.8.8"\]/g' /usr/share/containers/containers.conf

  # create libvirt network with dns server (only recursive)
  - name: create libvirt network
    shell: "(virsh net-destroy bmh || true) && (virsh net-undefine bmh || true) && virsh net-define {{ src_dir}}/test/e2e/libvirt/network-bmh-onlynameserver.xml"
  - name: start libvirt network
    shell: virsh net-start bmh

  - name: create resolv.conf.tmp
    shell: cat /dev/null > /etc/resolv.conf.tmp
  - name: Set DNS nameservers in /etc/resolv.conf
    blockinfile:
      path: /etc/resolv.conf
      block: |
        nameserver 192.168.222.1
        nameserver 8.8.8.8

  # start sushy-tools
  - name: Creating sushy-tools-config
    copy:
      dest: "/tmp/sushy-emulator.conf"
      content: |
       # Listen on 192.168.222.1:8000
       SUSHY_EMULATOR_LISTEN_IP = u"192.168.222.1"
       SUSHY_EMULATOR_LISTEN_PORT = 8000
       # The libvirt URI to use. This option enables libvirt driver.
       SUSHY_EMULATOR_LIBVIRT_URI = u"qemu:///system"
  - name: start sushytools
    shell: |
      podman rm -f sushy-tools && podman run --name sushy-tools --rm --network host --privileged -d \
      -v /var/run/libvirt:/var/run/libvirt:z \
      -v "/tmp/sushy-emulator.conf:/etc/sushy/sushy-emulator.conf:z" \
      -e SUSHY_EMULATOR_CONFIG=/etc/sushy/sushy-emulator.conf \
      quay.io/metal3-io/sushy-tools:latest sushy-emulator

  # install kind
  - name: install kind
    shell: curl -Lo ./kind https://kind.sigs.k8s.io/dl/{{ kind_version }}/kind-linux-amd64 && chmod +x ./kind && mv ./kind /usr/local/bin/kind
  - name: delete kind cluster
    shell: "kind delete cluster --name {{ kind_cluster_name }}"
  - name: create kind cluster
    shell: 'export INTERNAL_IP=$(cat /tmp/internal-ip) && cat {{ src_dir }}/test/kind.yaml | sed "s/<DEVICE_IP>/${INTERNAL_IP}/g" > /tmp/kind_with_ip.yaml && kind create cluster --name {{ kind_cluster_name }} --config /tmp/kind_with_ip.yaml'

  - name: build images
    shell: "export CONTAINER_TAG={{ container_tag }} && cd {{ src_dir }} && setsid make docker-build-all"

  - name: load images into kind
    shell: "podman save quay.io/edge-infrastructure/openshift-capi-agent-{{ item }}:{{ container_tag }} > {{ item }}.tar.gz && kind load image-archive --name {{ kind_cluster_name }} {{ item }}.tar.gz"
    loop:
      - controlplane
      - bootstrap

  - name: install kubectl
    shell: curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && mv kubectl /usr/local/bin && chmod +x /usr/local/bin/kubectl


  - name: deploy cert-manager
    shell: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/{{ cert_manager_version }}/cert-manager.yaml

  - name: wait for cert-manager
    shell: kubectl wait --for=condition=available deployment/cert-manager-webhook --timeout=600s -n cert-manager


  - name: deploy ironic
    shell: kubectl apply -k "{{ src_dir }}/test/e2e/manifests/ironic"
  - name: wait for ironic
    shell: kubectl wait --for=condition=available deployment/ironic --timeout=600s -n baremetal-operator-system

  - name: deploy BMO
    shell: kubectl apply -k "{{ src_dir }}/test/e2e/manifests/bmo"
  - name: wait for BMO
    shell: kubectl wait --for=condition=available deployment/baremetal-operator-controller-manager --timeout=600s -n baremetal-operator-system

  - name: deploy metallb
    shell: kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml
  - name: wait for metallb
    shell: kubectl wait --for=condition=available deployment/controller --timeout=300s -n metallb-system
  - name: deploy metallb
    shell: kubectl apply -f "{{ src_dir }}/test/e2e/manifests/metallb"

  - name: create nginx-ingress ns
    shell: kubectl create ns nginx-ingress
  - name: deploy nginx-ingress
    shell: kubectl -n nginx-ingress apply -f "{{ src_dir }}/test/e2e/manifests/ingress-nginx"
  - name: wait for load balancer ip
    shell: bash -c 'external_ip=""; while [ -z $external_ip ]; do echo "Waiting for end point..."; external_ip=$(kubectl -n nginx-ingress get svc ingress-nginx-controller --template="{{ "{{" }}range .status.loadBalancer.ingress{{ "}}" }}{{ "{{" }}.ip{{ "}}" }}{{ "{{" }}end{{ "}}" }}"); [ -z "$external_ip" ] && sleep 10; done; echo "End point ready-" && echo $external_ip > /tmp/loadbalancer_ip'
  - name: show lb IP
    shell: cat /tmp/loadbalancer_ip
    register: loadbalancerip
  - name: Debug tests output
    debug: "msg={{ loadbalancerip.stdout }}"

  - name: prepare infrastructure-operator on target
    shell: |
      envsubst < "{{ src_dir }}/test/e2e/manifests/infrastructure-operator/kustomization.yaml.tpl" > "{{ src_dir }}/test/e2e/manifests/infrastructure-operator/kustomization.yaml"
    environment:
      ASSISTED_SERVICE_IMAGE: "{{ lookup('ansible.builtin.env', 'ASSISTED_SERVICE_IMAGE', default='quay.io/edge-infrastructure/assisted-service@sha256:a78af04976f68f4e33b3a53001c959c15b7c350cb2b8836004066a40fd1e261d') }}"
      ASSISTED_IMAGE_SERVICE_IMAGE: "{{ lookup('ansible.builtin.env', 'ASSISTED_IMAGE_SERVICE_IMAGE', default='quay.io/edge-infrastructure/assisted-image-service@sha256:007f0b23d6e3f52837f44d3b4f02565e0121967e8b64e75dc38ba1b1d48e58c2') }}"
      ASSISTED_INSTALLER_AGENT_IMAGE: "{{ lookup('ansible.builtin.env', 'ASSISTED_INSTALLER_AGENT_IMAGE', default='quay.io/edge-infrastructure/assisted-installer-agent@sha256:b9cf4a9246eb3eb2d71dc17f6034c875be9c7d63120fa5a683b975dd878e6b20') }}"
      ASSISTED_INSTALLER_CONTROLLER_IMAGE: "{{ lookup('ansible.builtin.env', 'ASSISTED_INSTALLER_CONTROLLER_IMAGE', default='quay.io/edge-infrastructure/assisted-installer-controller@sha256:6fd6d481b66b78853214fbce28acbd45fd9345480c70b8775078ce44ba3d2ab8') }}"
      ASSISTED_INSTALLER_IMAGE: "{{ lookup('ansible.builtin.env', 'ASSISTED_INSTALLER_IMAGE', default='quay.io/edge-infrastructure/assisted-installer@sha256:7c4e456c16d9f1171a8328485e89d63622a5d1bc24c6e553031119b8027f3384') }}"
  - name: deploy infra-operator
    shell: kubectl apply -k "{{ src_dir }}/test/e2e/manifests/infrastructure-operator"
  - name: deploy agentserviceconfig
    shell: kubectl apply -f "{{ src_dir }}/test/e2e/manifests/infrastructure-operator/agentconfig.yaml"
  - name: wait for assisted-service deployment
    shell: "until kubectl get -n assisted-installer deployment assisted-service; do sleep 1; done"
  - name: wait for assisted service
    shell: kubectl wait --for=condition=available deployment/assisted-service --timeout=600s -n assisted-installer
  - name: wait for assisted-image-service
    shell: kubectl wait --for=condition=ready pod/assisted-image-service-0 --timeout=600s -n assisted-installer
  - name: install clusterctl
    shell: "curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/{{ capi_version }}/clusterctl-linux-amd64 -o clusterctl && install -o root -g root -m 0755 clusterctl /usr/local/bin/clusterctl"
  - name: deploy CAPI
    shell: "clusterctl init --core cluster-api:{{ capi_version }} --bootstrap - --control-plane - --infrastructure metal3:{{ capm3_version }}"

  # files generated before ansible
  - name: copy distfiles
    copy:
      src: "{{ dist_dir }}"
      dest: "/tmp"
  - name: set imagePullPolicy CAPBCOA
    shell: "cat /tmp/dist/bootstrap_install.yaml | sed 's/imagePullPolicy: Always/imagePullPolicy: Never/g' > /tmp/bootstrap_install.yaml && cat /tmp/dist/controlplane_install.yaml | sed 's/imagePullPolicy: Always/imagePullPolicy: Never/g' > /tmp/controlplane_install.yaml"

  - name: deploy CAPBCOA
    shell: kubectl apply -f /tmp/controlplane_install.yaml && kubectl apply -f /tmp/bootstrap_install.yaml

    # create libvirt BMH network with dns server
  - name: create libvirt network
    shell: export LOADBALANCER_IP=$(cat /tmp/loadbalancer_ip) && cat "{{ src_dir}}/test/e2e/libvirt/network-bmh.xml.tpl" | sed "s/<LOADBALANCER_IP>/${LOADBALANCER_IP}/g" >/tmp/libvirt-network-bmh.xml && (virsh net-destroy bmh || true) && (virsh net-undefine bmh || true) && virsh net-define /tmp/libvirt-network-bmh.xml
  - name: start libvirt network
    shell: virsh net-start bmh

    # create VMs
  - name: Copy VM creation script
    copy:
      dest: "/tmp/vm_functions"
      content: |
        #!/bin/bash
        function create_vm {
          name=$1
          mac=$2
          virsh destroy "${name}" 2>/dev/null || true
          virsh undefine --domain "${name}" --remove-all-storage --nvram 2>/dev/null || true
          virt-install -n "${name}" --pxe --os-variant=rhel8.0 --ram=16384 --vcpus=8 --network network=bmh,mac="${mac}" --disk size=120,bus=scsi,sparse=yes --check disk_size=off --noautoconsole
        }

  - name: Create BMHs
    shell: 'source /tmp/vm_functions && create_vm bmh-vm-{{ item }} "00:60:2f:31:81:{{ item }}"'
    with_sequence: count="{{ number_of_nodes }}" format="%02x"

  - name: create test namespace
    shell: "kubectl create ns {{ test_namespace }}"

  # create BMHs
  - name: Copy BMH creation script
    copy:
      dest: "/tmp/create_bmhs"
      content: |
        #!/bin/bash
        i=0
        for systemid in $(curl -s 192.168.222.1:8000/redfish/v1/Systems | jq -r '.Members[]."@odata.id"'); do
          echo "starting VM BMH..."
          i=$((i+1))
          name=$(curl -s 192.168.222.1:8000${systemid} | jq -r '.Name')
          sed -r "s%redfish-virtualmedia.*REPLACE_ID%redfish-virtualmedia+http://192.168.222.1:8000${systemid}%" {{ src_dir}}/test/e2e/bmh/bmh.yaml.tpl | sed -r "s/REPLACE_NAME/${name}/g" | sed -r "s/REPLACE_MAC/00:60:2f:31:81:0${name:0-1}/g" | kubectl -n {{ test_namespace }} apply -f -
          echo "done"
        done

  - name: create bmhs
    shell: chmod +x /tmp/create_bmhs && /tmp/create_bmhs
  - name: wait until BMHs
    shell: "until kubectl get -n {{ test_namespace }} bmh; do sleep 1; done"
  - name: wait for BMHs to be available
    shell: "kubectl -n {{ test_namespace }} wait --for=jsonpath='{.status.provisioning.state}'=available bmh/bmh-vm-{{ item }}"
    with_sequence: count="{{ number_of_nodes }}" format="%02x"

  # setup test env vars
  - name: Clean current test vars
    command: rm -rf ~/.test-config
  - name: Define SSH_AUTHORIZED_KEY
    lineinfile:
      path: ~/.test-config
      line: "export SSH_AUTHORIZED_KEY='{{ ssh_authorized_key }}'"
      create: true
      state: present
  - name: Define PULLSECRET
    lineinfile:
      path: ~/.test-config
      line: "export PULLSECRET='{{ pullsecret }}'"
      create: true
      state: present

  # create copy example manifest
  - name: generate manifest with vars
    shell: source ~/.test-config && cat "{{ src_dir }}/examples/{{ example_manifest }}" | sed 's|<PULLSECRET>|'"${PULLSECRET}"'|g' | sed 's|<SSH_AUTHORIZED_KEY>|'"${SSH_AUTHORIZED_KEY}"'|g' > /tmp/example_manifest.yaml

  - name: apply example
    shell: kubectl -n {{ test_namespace }} apply -f /tmp/example_manifest.yaml
  - name: wait for ACI
    shell: "until kubectl get -n {{ test_namespace }} aci {{ cluster_name }}; do sleep 1; done"

  - name: wait for ACI to be installing
    shell: "kubectl -n {{ test_namespace }} wait --for=jsonpath='{.status.debugInfo.state}'=installing aci/{{ cluster_name }} --timeout=1200s"

  - name: wait controlplane to be ready
    shell: "kubectl wait -n {{ test_namespace }} openshiftassistedcontrolplane/{{ cluster_name }} --for condition=Ready --timeout 3600s"

  - name: make sure ACI to be installed
    shell: "kubectl -n {{ test_namespace }} wait --for=jsonpath='{.status.debugInfo.state}'=adding-hosts aci/{{ cluster_name }} --timeout=3600s"

  - name: upgrade cluster
    shell: "kubectl -n {{ test_namespace }} patch openshiftassistedcontrolplane {{ cluster_name }} --type=merge --patch '{\"spec\": {\"distributionVersion\": \"{{ upgrade_to_version }}\"}}'"

  - name: wait for upgrade to kick-in
    shell: "sleep 60"

  # wait ctrlplane to complete upgrade
  - name: wait controlplane to complete upgrade
    shell: "kubectl wait -n {{ test_namespace }} openshiftassistedcontrolplane/{{ cluster_name }} --for condition=UpgradeCompleted --timeout 3600s"

  # wait ctrlplane to be ready
  - name: wait controlplane to be ready
    shell: "kubectl wait -n {{ test_namespace }} openshiftassistedcontrolplane/{{ cluster_name }} --for condition=Ready --timeout 7200s"
