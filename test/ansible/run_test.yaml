---
# TEST replace 127.0.0.1 with internal IP

- name: Setup and run tests
  hosts: test_runner
  vars:
    distrobox_name: assisted-capi-providers
    kind_version: v0.25.0
    cert_manager_version: v1.5.3
    remote_manifests_path: /tmp/manifests
    test_namespace: test-capi
    example_manifest: multi-node-example.yaml
    capi_version: v1.7.1
    capm3_version: v1.6.0
    dist_dir: "{{ lookup('ansible.builtin.env', 'DIST_DIR') }}"
    src_dir: /tmp/capbcoa
    kind_cluster_name: capi-baremetal-provider
  tasks:
  - name: Install EPEL
    ansible.builtin.dnf:
      enablerepo: powertools
      name:
      - epel-release
  - name: Install dependencies
    command: dnf --enablerepo=crb -y install git make podman distrobox qemu-kvm libvirt virt-install containernetworking-plugins
  - name: enable libvirtd
    command: systemctl enable --now libvirtd

  # remove all other copies once CI branch is merged in app repo
  - name: copy src
    synchronize:
      src: ../../../
      dest: "{{ src_dir }}"
      archive: true
      recursive: true

  - name: Detect internal IP
    shell: hostname -i | grep -oE "10\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}" | head -n1 > /tmp/internal-ip
  - name: Show internal IP
    shell: cat /tmp/internal-ip
    register: internalip
  - name: Debug tests output
    debug: "msg={{ internalip.stdout }}"

  # set dns server to 192.168.222.1 in containers
  - name: change default dns_servers for containers
    shell: sed -i 's/#dns_servers = \[\]/dns_servers = \["192.168.222.1", "8.8.8.8"\]/g' /usr/share/containers/containers.conf
    # create libvirt network with dns server (only recursive)
  - name: create libvirt network
    shell: "(virsh net-destroy bmh || true) && (virsh net-undefine bmh || true) && virsh net-define {{ src_dir}}/test/e2e/libvirt/network-bmh-onlynameserver.xml"
  - name: start libvirt network
    shell: virsh net-start bmh
  # set dns server in
  - name: create resolv.conf.tmp
    shell: cat /dev/null > /etc/resolv.conf.tmp
  - name: Set DNS nameservers in /etc/resolv.conf
    blockinfile:
      path: /etc/resolv.conf.tmp
      block: |
        nameserver 192.168.222.1
        nameserver 8.8.8.8

  # start sushy-tools
  - name: Creating sushy-tools-config
    copy:
      dest: "/tmp/sushy-emulator.conf"
      content: |
       # Listen on 192.168.222.1:8000
       SUSHY_EMULATOR_LISTEN_IP = u"192.168.222.1"
       SUSHY_EMULATOR_LISTEN_PORT = 8000
       # The libvirt URI to use. This option enables libvirt driver.
       SUSHY_EMULATOR_LIBVIRT_URI = u"qemu:///system"
       SUSHY_EMULATOR_IGNORE_BOOT_DEVICE = True
  - name: start sushytools
    shell: |
      podman rm -f sushy-tools && podman run --name sushy-tools --rm --network host --privileged -d \
      -v /var/run/libvirt:/var/run/libvirt:z \
      -v "/tmp/sushy-emulator.conf:/etc/sushy/sushy-emulator.conf:z" \
      -e SUSHY_EMULATOR_CONFIG=/etc/sushy/sushy-emulator.conf \
      quay.io/metal3-io/sushy-tools:latest sushy-emulator

  # why backing up if we keep overriding
  - name: backup current resolv.conf
    shell: cp /etc/resolv.conf /etc/resolv.conf.bak
  - name: replace resolv.conf
    shell: cp /etc/resolv.conf.tmp /etc/resolv.conf

  # install kind
  - name: install kind
    shell: curl -Lo ./kind https://kind.sigs.k8s.io/dl/{{ kind_version }}/kind-linux-amd64 && chmod +x ./kind && mv ./kind /usr/local/bin/kind
  - name: delete kind cluster
    shell: "kind delete cluster --name {{ kind_cluster_name }}"
  - name: create kind cluster
    shell: 'export INTERNAL_IP=$(cat /tmp/internal-ip) && cat {{ src_dir }}/test/kind.yaml | sed "s/<DEVICE_IP>/${INTERNAL_IP}/g" > /tmp/kind_with_ip.yaml && kind create cluster --name {{ kind_cluster_name }} --config /tmp/kind_with_ip.yaml'

  - name: build images
    shell: "export CONTAINER_TAG=local && cd {{ src_dir }} && setsid make docker-build-all"

  - name: load images into kind
    shell: "podman save quay.io/edge-infrastructure/openshift-capi-agent-{{ item }}:local > {{ item }}.tar.gz && kind load image-archive --name {{ kind_cluster_name }} {{ item }}.tar.gz"
    loop:
      - controlplane
      - bootstrap

  - name: install kubectl
    shell: curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" && mv kubectl /usr/local/bin && chmod +x /usr/local/bin/kubectl


  - name: deploy cert-manager
    shell: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/{{ cert_manager_version }}/cert-manager.yaml

  - name: wait for cert-manager
    shell: kubectl wait --for=condition=available deployment/cert-manager-webhook --timeout=600s -n cert-manager


  - name: deploy ironic
    shell: kubectl apply -k "{{ src_dir }}/test/e2e/manifests/ironic"
  - name: wait for ironic
    shell: kubectl wait --for=condition=available deployment/ironic --timeout=600s -n baremetal-operator-system

  - name: deploy BMO
    shell: kubectl apply -k "{{ src_dir }}/test/e2e/manifests/bmo"
  - name: wait for BMO
    shell: kubectl wait --for=condition=available deployment/baremetal-operator-controller-manager --timeout=600s -n baremetal-operator-system

  - name: deploy metallb
    shell: kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.7/config/manifests/metallb-native.yaml
  - name: wait for metallb
    shell: kubectl wait --for=condition=available deployment/controller --timeout=300s -n metallb-system
  - name: deploy metallb
    shell: kubectl apply -f "{{ src_dir }}/test/e2e/manifests/metallb"

  - name: create nginx-ingress ns
    shell: kubectl create ns nginx-ingress
  - name: deploy nginx-ingress
    shell: kubectl -n nginx-ingress apply -f "{{ src_dir }}/test/e2e/manifests/ingress-nginx"
  - name: wait for load balancer ip
    shell: bash -c 'external_ip=""; while [ -z $external_ip ]; do echo "Waiting for end point..."; external_ip=$(kubectl -n nginx-ingress get svc ingress-nginx-controller --template="{{ "{{" }}range .status.loadBalancer.ingress{{ "}}" }}{{ "{{" }}.ip{{ "}}" }}{{ "{{" }}end{{ "}}" }}"); [ -z "$external_ip" ] && sleep 10; done; echo "End point ready-" && echo $external_ip > /tmp/loadbalancer_ip'
  - name: show lb IP
    shell: cat /tmp/loadbalancer_ip
    register: loadbalancerip
  - name: Debug tests output
    debug: "msg={{ loadbalancerip.stdout }}"

  - name: deploy infra-operator
    shell: kubectl apply -k "{{ src_dir }}/test/e2e/manifests/infrastructure-operator"
  - name: deploy agentserviceconfig
    shell: kubectl apply -f "{{ src_dir }}/test/e2e/manifests/infrastructure-operator/agentconfig.yaml"
  - name: wait for assisted-service deployment
    shell: "until kubectl get -n assisted-installer deployment assisted-service; do sleep 1; done"
  - name: wait for assisted service
    shell: kubectl wait --for=condition=available deployment/assisted-service --timeout=600s -n assisted-installer
  - name: wait for assisted-image-service
    shell: kubectl wait --for=condition=ready pod/assisted-image-service-0 --timeout=600s -n assisted-installer
  - name: install clusterctl
    shell: curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.8.5/clusterctl-linux-amd64 -o clusterctl && install -o root -g root -m 0755 clusterctl /usr/local/bin/clusterctl
  - name: deploy CAPI
    shell: "clusterctl init --core cluster-api:{{ capi_version }} --bootstrap - --control-plane - --infrastructure metal3:{{ capm3_version }}"

  # files generated before ansible
  - name: copy distfiles
    copy:
      src: ../../dist
      dest: "{{ src_dir }}"
  - name: set imagePullPolicy CAPBCOA
    shell: "cat {{ src_dir }}/dist/bootstrap_install.yaml | sed 's/imagePullPolicy: Always/imagePullPolicy: Never/g' > /tmp/bootstrap_install.yaml && cat {{ src_dir }}/dist/controlplane_install.yaml | sed 's/imagePullPolicy: Always/imagePullPolicy: Never/g' > /tmp/controlplane_install.yaml"

  - name: deploy CAPBCOA
    shell: kubectl apply -f /tmp/controlplane_install.yaml && kubectl apply -f /tmp/bootstrap_install.yaml

    # create libvirt BMH network with dns server
  - name: create libvirt network
    shell: export LOADBALANCER_IP=$(cat /tmp/loadbalancer_ip) && cat "{{ src_dir}}/test/e2e/libvirt/network-bmh.xml.tpl" | sed "s/<LOADBALANCER_IP>/${LOADBALANCER_IP}/g" >/tmp/libvirt-network-bmh.xml && (virsh net-destroy bmh || true) && (virsh net-undefine bmh || true) && virsh net-define /tmp/libvirt-network-bmh.xml
  - name: start libvirt network
    shell: virsh net-start bmh

    # create BMHs
  - name: Copy BMH creation script
    copy:
      dest: "/tmp/bmh_functions"
      content: |
        #!/bin/bash
        function create_vm {
          name=$1
          mac=$2
          virsh destroy "${name}" 2>/dev/null || true
          virsh undefine --domain "${name}" --remove-all-storage --nvram 2>/dev/null || true
          virt-install -n "${name}" --pxe --os-variant=rhel8.0 --ram=16384 --vcpus=8 --network network=bmh,mac="${mac}" --disk size=120,bus=scsi,sparse=yes --check disk_size=off --noautoconsole
        }

  - name: Create BMHs
    shell: "source /tmp/bmh_functions && create_vm bmh-vm-{{ item.id }} {{ item.mac }}"
    loop:
      - {id: "01", mac: "00:60:2f:31:81:01"}
      - {id: "02", mac: "00:60:2f:31:81:02"}
      - {id: "03", mac: "00:60:2f:31:81:03"}
      - {id: "04", mac: "00:60:2f:31:81:04"}
      - {id: "05", mac: "00:60:2f:31:81:05"}

  - name: create test namespace
    shell: "kubectl create ns {{ test_namespace }}"
  # create BMHs
  - name: Copy BMH creation script
    copy:
      dest: "/tmp/create_bmhs"
      content: |
        #!/bin/bash
        i=0
        for systemid in $(curl -s 192.168.222.1:8000/redfish/v1/Systems | jq -r '.Members[]."@odata.id"'); do
          echo "starting VM BMH..."
          i=$((i+1))
          name=$(curl -s 192.168.222.1:8000${systemid} | jq -r '.Name')
          sed -r "s%redfish-virtualmedia.*REPLACE_ID%redfish-virtualmedia+http://192.168.222.1:8000${systemid}%" {{ src_dir}}/test/e2e/bmh/bmh.yaml.tpl | sed -r "s/REPLACE_NAME/${name}/g" | sed -r "s/REPLACE_MAC/00:60:2f:31:81:0${name:0-1}/g" | kubectl -n {{ test_namespace }} apply -f -
          echo "done"
        done

  - name: create bmhs
    shell: chmod +x /tmp/create_bmhs && /tmp/create_bmhs
  - name: wait for BMHs to be available
    shell: "kubectl -n {{ test_namespace }} wait --for=jsonpath='{.status.provisioning.state}'=available bmh/bmh-vm-{{ item.id }}"
    loop:
      - { id: "01" }
      - { id: "02" }
      - { id: "03" }
      - { id: "04" }
      - { id: "05" }

  # setup test env vars
  - name: Clean current test vars
    command: rm -rf ~/.test-config
  - name: Define SSH_AUTHORIZED_KEY
    lineinfile:
      path: ~/.test-config
      line: "export SSH_AUTHORIZED_KEY='ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDZnhCSkG1N4RkRKD+0XxoXfzMuTysmQUSmZ3UY4VOWZQsC5i4J75VKg4BhXmdEHL1ibW1b2Ys58RDjeOTBPu2tE93BpKPgfUqWAcqF/3Ecf3BRpdc2Uj0SY6ELJeyXEBze1ODDja/rI0QRPRmd4y0VuUlrGJlBOugcwA88ECigEnSDU9M3UgMHaAjA+HyD7X7mv41CIHZMUES/0YB0m7UGP1GMPpbPxdDx4DtZ1CC+ba1LyNm7Kdv8pxj2lFfLpDW4BZF3p33mlRyO6AfBJn/zpjrt5q6iuf4IzRxiu3R0x1Y34hgSRPYWH3gDM5lBnfqqdycr32uWT/gibLqBdHqH3lJBjOhjs7AxQ1Ll74BZ0ngUL64lrpJ7OzLXsruOc0uFsuNBp37Uu9FK7q+lHyO4nURyK6yeXtpOoVPJn0VleNNSjL6aKDvxxAO78MsKDLgn0cthAmLRw6YOZHPOIQze4yUkaM+DrIPc/HgOQpJXtUqArjz9X2JPn8OBmVQzjV0= root@rdu-infra-edge-03.infra-edge.lab.eng.rdu2.redhat.com'"
      create: true
      state: present
  - name: Define PULLSECRET
    lineinfile:
      path: ~/.test-config
      line: "export PULLSECRET='{{ lookup('ansible.builtin.env', 'PULLSECRET') }}'"
      create: true
      state: present

  # create copy example manifest
  - name: generate manifest with vars
    shell: source ~/.test-config && cat "{{ src_dir }}/examples/{{ example_manifest }}" | sed 's|<PULLSECRET>|'"${PULLSECRET}"'|g' | sed 's|<SSH_AUTHORIZED_KEY>|'"${SSH_AUTHORIZED_KEY}"'|g' > /tmp/example_manifest.yaml

  - name: apply example
    shell: kubectl -n {{ test_namespace }} apply -f /tmp/example_manifest.yaml
  - name: wait for ACI
    shell: "until kubectl get -n {{ test_namespace }} aci test-multinode; do sleep 1; done"

  - name: wait for ACI to be installing
    shell: "kubectl -n {{ test_namespace }} wait --for=jsonpath='{.status.debugInfo.state}'=installing aci/test-multinode --timeout=1200s"

  - name: wait for ACI to be installed
    shell: "kubectl -n {{ test_namespace }} wait --for=jsonpath='{.status.debugInfo.state}'=adding-hosts aci/test-multinode --timeout=3600s"
